{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
      "     -------------------------------------- 867.6/867.6 KB 1.3 MB/s eta 0:00:00\n",
      "Collecting deeplake\n",
      "  Downloading deeplake-3.9.3.tar.gz (590 kB)\n",
      "     -------------------------------------- 590.0/590.0 KB 1.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting openai\n",
      "  Downloading openai-1.25.1-py3-none-any.whl (312 kB)\n",
      "     -------------------------------------- 312.9/312.9 KB 3.9 MB/s eta 0:00:00\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp39-cp39-win_amd64.whl (798 kB)\n",
      "     -------------------------------------- 798.7/798.7 KB 3.2 MB/s eta 0:00:00\n",
      "Collecting numpy<2,>=1\n",
      "  Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting requests<3,>=2\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.9.5-cp39-cp39-win_amd64.whl (371 kB)\n",
      "     -------------------------------------- 371.6/371.6 KB 3.9 MB/s eta 0:00:00\n",
      "Collecting langchain-community<0.1,>=0.0.36\n",
      "  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 2.3 MB/s eta 0:00:00\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.48\n",
      "  Downloading langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
      "     -------------------------------------- 302.8/302.8 KB 6.2 MB/s eta 0:00:00\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.29-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 2.3 MB/s eta 0:00:00\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Downloading langsmith-0.1.54-py3-none-any.whl (116 kB)\n",
      "     -------------------------------------- 116.7/116.7 KB 1.1 MB/s eta 0:00:00\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting pydantic<3,>=1\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "     -------------------------------------- 409.3/409.3 KB 2.3 MB/s eta 0:00:00\n",
      "Collecting PyYAML>=5.3\n",
      "  Using cached PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting pyjwt\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.34.98-py3-none-any.whl (139 kB)\n",
      "     ------------------------------------ 139.3/139.3 KB 922.0 kB/s eta 0:00:00\n",
      "Collecting pillow~=10.2.0\n",
      "  Downloading pillow-10.2.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 1.2 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.3/78.3 KB 2.2 MB/s eta 0:00:00\n",
      "Collecting lz4\n",
      "  Downloading lz4-4.3.3-cp39-cp39-win_amd64.whl (99 kB)\n",
      "     ---------------------------------------- 99.8/99.8 KB 1.9 MB/s eta 0:00:00\n",
      "Collecting pathos\n",
      "  Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
      "     ---------------------------------------- 82.1/82.1 KB 2.3 MB/s eta 0:00:00\n",
      "Collecting humbug>=0.3.1\n",
      "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "     ---------------------------------------- 85.6/85.6 KB 1.6 MB/s eta 0:00:00\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\admin\\onedrive\\máy tính\\project\\python\\chat-with-code\\.venv\\lib\\site-packages (from openai) (4.11.0)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "     ---------------------------------------- 75.6/75.6 KB 1.4 MB/s eta 0:00:00\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2024.4.28-cp39-cp39-win_amd64.whl (268 kB)\n",
      "     -------------------------------------- 269.0/269.0 KB 1.4 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.5-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "     ---------------------------------------- 60.8/60.8 KB 1.6 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n",
      "Collecting idna>=2.8\n",
      "  Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "     -------------------------------------- 66.8/66.8 KB 898.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\admin\\onedrive\\máy tính\\project\\python\\chat-with-code\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "     -------------------------------------- 49.3/49.3 KB 831.0 kB/s eta 0:00:00\n",
      "Collecting certifi\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "     -------------------------------------- 163.8/163.8 KB 1.6 MB/s eta 0:00:00\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.9/77.9 KB 1.4 MB/s eta 0:00:00\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting packaging<24.0,>=23.2\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.3-cp39-none-win_amd64.whl (138 kB)\n",
      "     -------------------------------------- 138.6/138.6 KB 1.2 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.18.2\n",
      "  Downloading pydantic_core-2.18.2-cp39-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 932.3 kB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "     ------------------------------------ 100.4/100.4 KB 639.3 kB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "     ------------------------------------ 121.1/121.1 KB 886.6 kB/s eta 0:00:00\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-3.0.3-cp39-cp39-win_amd64.whl (290 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\onedrive\\máy tính\\project\\python\\chat-with-code\\.venv\\lib\\site-packages (from tqdm->deeplake) (0.4.6)\n",
      "Collecting botocore<1.35.0,>=1.34.98\n",
      "  Downloading botocore-1.34.98-py3-none-any.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 1.2 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "     ---------------------------------------- 82.2/82.2 KB 2.3 MB/s eta 0:00:00\n",
      "Collecting ppft>=1.7.6.8\n",
      "  Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 KB 1.5 MB/s eta 0:00:00\n",
      "Collecting pox>=0.3.4\n",
      "  Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
      "Collecting multiprocess>=0.70.16\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.4/133.4 KB 2.6 MB/s eta 0:00:00\n",
      "Collecting dill>=0.3.8\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     -------------------------------------- 116.3/116.3 KB 2.3 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "     -------------------------------------- 143.8/143.8 KB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\admin\\onedrive\\máy tính\\project\\python\\chat-with-code\\.venv\\lib\\site-packages (from botocore<1.35.0,>=1.34.98->boto3->deeplake) (2.9.0.post0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\onedrive\\máy tính\\project\\python\\chat-with-code\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.98->boto3->deeplake) (1.16.0)\n",
      "Building wheels for collected packages: deeplake\n",
      "  Building wheel for deeplake (pyproject.toml): started\n",
      "  Building wheel for deeplake (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for deeplake: filename=deeplake-3.9.3-py3-none-any.whl size=711837 sha256=994c1224fb5741c133d1ed24895eedb5568e94cd34d5951997188b063a6c063e\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\4b\\24\\69\\5ae83b629862fee004232070633717f99ce0bb05036e7b0162\n",
      "Successfully built deeplake\n",
      "Installing collected packages: urllib3, tqdm, tenacity, sniffio, regex, PyYAML, pyjwt, pydantic-core, ppft, pox, pillow, packaging, orjson, numpy, mypy-extensions, multidict, lz4, jsonpointer, jmespath, idna, h11, greenlet, frozenlist, distro, dill, click, charset-normalizer, certifi, attrs, async-timeout, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic, multiprocess, marshmallow, jsonpatch, httpcore, botocore, anyio, aiosignal, tiktoken, s3transfer, pathos, langsmith, humbug, httpx, dataclasses-json, aiohttp, openai, langchain-core, boto3, langchain-text-splitters, langchain-community, deeplake, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.29 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.3.0 async-timeout-4.0.3 attrs-23.2.0 boto3-1.34.98 botocore-1.34.98 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 dataclasses-json-0.6.5 deeplake-3.9.3 dill-0.3.8 distro-1.9.0 frozenlist-1.4.1 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 humbug-0.3.2 idna-3.7 jmespath-1.0.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.17 langchain-community-0.0.36 langchain-core-0.1.50 langchain-text-splitters-0.0.1 langsmith-0.1.54 lz4-4.3.3 marshmallow-3.21.2 multidict-6.0.5 multiprocess-0.70.16 mypy-extensions-1.0.0 numpy-1.26.4 openai-1.25.1 orjson-3.10.3 packaging-23.2 pathos-0.3.2 pillow-10.2.0 pox-0.3.4 ppft-1.7.6.8 pydantic-2.7.1 pydantic-core-2.18.2 pyjwt-2.8.0 regex-2024.4.28 requests-2.31.0 s3transfer-0.10.1 sniffio-1.3.1 tenacity-8.2.3 tiktoken-0.6.0 tqdm-4.66.4 typing-inspect-0.9.0 urllib3-1.26.18 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain deeplake openai tiktoken\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(os.curdir, \".env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small', disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'codebase'...\n"
     ]
    }
   ],
   "source": [
    "!mkdir codebase && git clone https://github.com/VainF/Torch-Pruning codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.gitignore', 'AUTHORS', 'LICENSE', 'practical_structural_pruning.md', 'README.md', 'README_CN.md', 'requirements.txt', 'setup.py']\n",
      "['config', 'description', 'HEAD', 'index', 'packed-refs']\n",
      "['applypatch-msg.sample', 'commit-msg.sample', 'fsmonitor-watchman.sample', 'post-update.sample', 'pre-applypatch.sample', 'pre-commit.sample', 'pre-merge-commit.sample', 'pre-push.sample', 'pre-rebase.sample', 'pre-receive.sample', 'prepare-commit-msg.sample', 'push-to-checkout.sample', 'update.sample']\n",
      "['exclude']\n",
      "['HEAD']\n",
      "[]\n",
      "['master']\n",
      "[]\n",
      "['HEAD']\n",
      "[]\n",
      "[]\n",
      "['pack-24590129ea2acde2a3a84b9c8aaedd58a82ee5d1.idx', 'pack-24590129ea2acde2a3a84b9c8aaedd58a82ee5d1.pack']\n",
      "[]\n",
      "['master']\n",
      "[]\n",
      "['HEAD']\n",
      "[]\n",
      "[]\n",
      "['python-publish.yml', 'test_torch_1110.yml', 'test_torch_181.yml', 'test_torch_1_12_1.yml', 'test_torch_200.yml']\n",
      "['concat.png', 'conv-conv.png', 'conv-fc.png', 'densenet_dep.png', 'dep.png', 'dep1.png', 'dep2.png', 'dep3.png', 'group_sparsity.png', 'intro.jpg', 'intro.png', 'residual.png', 'split.png']\n",
      "['benchmark_importance_criteria.py', 'benchmark_latency.py', 'main.py', 'main_imagenet.py', 'readme.md', 'registry.py', 'requirements.txt']\n",
      "['__init__.py']\n",
      "['__init__.py']\n",
      "['densenet.py', 'googlenet.py', 'inceptionv3.py', 'inceptionv4.py', 'mobilenetv2.py', 'nasnet.py', 'preactresnet.py', 'resnet.py', 'resnet_tiny.py', 'resnext.py', 'senet.py', 'swin.py', 'vgg.py', 'vit.py', 'xception.py', '__init__.py']\n",
      "['dgcnn.py', '__init__.py']\n",
      "['vision_transformer.py', '__init__.py']\n",
      "['evaluator.py', 'metrics.py', 'utils.py', '__init__.py']\n",
      "['modelnet40.py', '__init__.py']\n",
      "['presets.py', 'sampler.py', 'transforms.py', 'utils.py', '__init__.py']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['cifar10-global-group_sl-resnet56.txt']\n",
      "['cifar10-global-group_sl-resnet56.txt']\n",
      "['cifar10-global-growing_reg-resnet56.txt']\n",
      "['cifar10-global-l1-resnet56.txt']\n",
      "['cifar10-global-slim-resnet56.txt']\n",
      "[]\n",
      "[]\n",
      "['cifar100-global-group_sl-vgg19.txt']\n",
      "[]\n",
      "['cifar_pretrain.sh']\n",
      "[]\n",
      "['global_group_norm.sh', 'global_group_sl.sh', 'global_group_sl_p2.sh', 'global_l1.sh', 'global_l1_group_conv.sh', 'global_lamp.sh', 'global_rand.sh', 'global_slim.sh', 'local_group_norm.sh', 'local_l1.sh', 'local_l1_group_conv.sh', 'local_rand.sh']\n",
      "['bn_pruner.sh', 'group_pruner.sh', 'l1_norm_pruner.sh']\n",
      "['densenet_gsl.sh', 'mobilenetv2_group_norm.sh', 'mobilenetv2_group_sl.sh', 'next50_group_norm.sh', 'next50_group_sl.sh', 'regnet_group_norm.sh', 'regnet_group_sl.sh', 'resnet50_group_norm.sh', 'resnet50_group_sl.sh', 'vgg_group_norm copy.sh', 'vit_group_norm.sh']\n",
      "['global_group_norm.sh']\n",
      "['global_group_norm.sh']\n",
      "['draw.py']\n",
      "[]\n",
      "['measure_latency.py']\n",
      "['0 - QuickStart.ipynb', '1 - Customize Your Own Pruners.ipynb', '2 - Exploring Dependency Groups.ipynb']\n",
      "['prune_timm_models.py', 'readme.md']\n",
      "['readme.md', 'torchvision_global_pruning.py', 'torchvision_pruning.py']\n",
      "['draw_acc_curve.py', 'finetune.py', 'measure_latency.py', 'presets.py', 'prune_hf_bert.py', 'prune_hf_swin.py', 'prune_hf_vit.py', 'prune_timm_vit.py', 'readme.md', 'sampler.py', 'transforms.py', 'utils.py']\n",
      "['finetune_hf_vit_b_16_l1_uniform.sh', 'finetune_hf_vit_b_16_taylor_uniform.sh', 'finetune_timm_deit_b_16_taylor_uniform.sh', 'finetune_timm_vit_b_16_hessian_uniform.sh', 'finetune_timm_vit_b_16_l1_uniform.sh', 'finetune_timm_vit_b_16_l2_uniform.sh', 'finetune_timm_vit_b_16_taylor_bottleneck.sh', 'finetune_timm_vit_b_16_taylor_uniform.sh', 'prune_hf_vit_b_16_l1_uniform.sh', 'prune_hf_vit_b_16_taylor_uniform.sh', 'prune_timm_deit_b_16_taylor_uniform.sh', 'prune_timm_vit_b_16_hessian_uniform.sh', 'prune_timm_vit_b_16_l1_uniform.sh', 'prune_timm_vit_b_16_l2_uniform.sh', 'prune_timm_vit_b_16_taylor_bottleneck.sh', 'prune_timm_vit_b_16_taylor_uniform.sh', 'prune_timm_vit_b_16_taylor_uniform_global.sh', 'test_pretrained_hf_vit_b_16.sh', 'test_pretrained_timm_deit_b_16.sh', 'test_pretrained_timm_vit_b_16.sh']\n",
      "['detect_after_pruning.py', 'readme.md']\n",
      "['readme.md', 'yolov7_detect_pruned.py', 'yolov7_train_pruned.py']\n",
      "['readme.md', 'yolov8_pruning.py']\n",
      "['graph_drawing.py', 'test_backward.py', 'test_benchmark.py', 'test_concat.py', 'test_concat_split.py', 'test_customized_layer.py', 'test_dependency_graph.py', 'test_dependency_lenet.py', 'test_flops.py', 'test_fully_connected_layers.py', 'test_group_prune.py', 'test_hessian_importance.py', 'test_importance_reduction.py', 'test_interactive_pruner.py', 'test_load.py', 'test_multiple_inputs_and_outputs.py', 'test_non_feature_dim_cat.py', 'test_pruner.py', 'test_pruning_fn.py', 'test_regularization.py', 'test_reshape.py', 'test_score_normalization.py', 'test_serialization.py', 'test_single_channel_output.py', 'test_soft_pruning.py', 'test_split.py', 'test_taylor_importance.py', 'test_unused_parameters.py', 'test_unwrapped_parameters.py']\n",
      "['dependency.py', 'ops.py', 'serialization.py', '_helpers.py', '__init__.py']\n",
      "['function.py', 'importance.py', '__init__.py']\n",
      "['batchnorm_scale_pruner.py', 'group_norm_pruner.py', 'growing_reg_pruner.py', 'magnitude_based_pruner.py', 'metapruner.py', 'scheduler.py', '__init__.py']\n",
      "['benchmark.py', 'compute_mat_grad.py', 'op_counter.py', 'utils.py', '__init__.py']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "root_dir = './codebase'\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            loader = TextLoader(os.path.join(dirpath, filename), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2136, which is longer than the specified 1000\n",
      "Created a chunk of size 1224, which is longer than the specified 1000\n",
      "Created a chunk of size 1005, which is longer than the specified 1000\n",
      "Created a chunk of size 1109, which is longer than the specified 1000\n",
      "Created a chunk of size 1483, which is longer than the specified 1000\n",
      "Created a chunk of size 1077, which is longer than the specified 1000\n",
      "Created a chunk of size 3192, which is longer than the specified 1000\n",
      "Created a chunk of size 1130, which is longer than the specified 1000\n",
      "Created a chunk of size 2439, which is longer than the specified 1000\n",
      "Created a chunk of size 1362, which is longer than the specified 1000\n",
      "Created a chunk of size 1524, which is longer than the specified 1000\n",
      "Created a chunk of size 3300, which is longer than the specified 1000\n",
      "Created a chunk of size 1020, which is longer than the specified 1000\n",
      "Created a chunk of size 1540, which is longer than the specified 1000\n",
      "Created a chunk of size 1219, which is longer than the specified 1000\n",
      "Created a chunk of size 1037, which is longer than the specified 1000\n",
      "Created a chunk of size 1638, which is longer than the specified 1000\n",
      "Created a chunk of size 3399, which is longer than the specified 1000\n",
      "Created a chunk of size 3680, which is longer than the specified 1000\n",
      "Created a chunk of size 2918, which is longer than the specified 1000\n",
      "Created a chunk of size 2452, which is longer than the specified 1000\n",
      "Created a chunk of size 2483, which is longer than the specified 1000\n",
      "Created a chunk of size 1107, which is longer than the specified 1000\n",
      "Created a chunk of size 1887, which is longer than the specified 1000\n",
      "Created a chunk of size 1129, which is longer than the specified 1000\n",
      "Created a chunk of size 2482, which is longer than the specified 1000\n",
      "Created a chunk of size 1166, which is longer than the specified 1000\n",
      "Created a chunk of size 1137, which is longer than the specified 1000\n",
      "Created a chunk of size 1641, which is longer than the specified 1000\n",
      "Created a chunk of size 1027, which is longer than the specified 1000\n",
      "Created a chunk of size 1317, which is longer than the specified 1000\n",
      "Created a chunk of size 1518, which is longer than the specified 1000\n",
      "Created a chunk of size 1034, which is longer than the specified 1000\n",
      "Created a chunk of size 1247, which is longer than the specified 1000\n",
      "Created a chunk of size 1839, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n",
      "Created a chunk of size 1560, which is longer than the specified 1000\n",
      "Created a chunk of size 2263, which is longer than the specified 1000\n",
      "Created a chunk of size 2028, which is longer than the specified 1000\n",
      "Created a chunk of size 2132, which is longer than the specified 1000\n",
      "Created a chunk of size 2264, which is longer than the specified 1000\n",
      "Created a chunk of size 2180, which is longer than the specified 1000\n",
      "Created a chunk of size 2132, which is longer than the specified 1000\n",
      "Created a chunk of size 1575, which is longer than the specified 1000\n",
      "Created a chunk of size 1473, which is longer than the specified 1000\n",
      "Created a chunk of size 1440, which is longer than the specified 1000\n",
      "Created a chunk of size 1533, which is longer than the specified 1000\n",
      "Created a chunk of size 3188, which is longer than the specified 1000\n",
      "Created a chunk of size 1533, which is longer than the specified 1000\n",
      "Created a chunk of size 2348, which is longer than the specified 1000\n",
      "Created a chunk of size 1257, which is longer than the specified 1000\n",
      "Created a chunk of size 1244, which is longer than the specified 1000\n",
      "Created a chunk of size 2190, which is longer than the specified 1000\n",
      "Created a chunk of size 1575, which is longer than the specified 1000\n",
      "Created a chunk of size 1420, which is longer than the specified 1000\n",
      "Created a chunk of size 1070, which is longer than the specified 1000\n",
      "Created a chunk of size 1076, which is longer than the specified 1000\n",
      "Created a chunk of size 2263, which is longer than the specified 1000\n",
      "Created a chunk of size 1985, which is longer than the specified 1000\n",
      "Created a chunk of size 1124, which is longer than the specified 1000\n",
      "Created a chunk of size 2853, which is longer than the specified 1000\n",
      "Created a chunk of size 1119, which is longer than the specified 1000\n",
      "Created a chunk of size 1891, which is longer than the specified 1000\n",
      "Created a chunk of size 1092, which is longer than the specified 1000\n",
      "Created a chunk of size 2956, which is longer than the specified 1000\n",
      "Created a chunk of size 1027, which is longer than the specified 1000\n",
      "Created a chunk of size 1290, which is longer than the specified 1000\n",
      "Created a chunk of size 1709, which is longer than the specified 1000\n",
      "Created a chunk of size 3614, which is longer than the specified 1000\n",
      "Created a chunk of size 1314, which is longer than the specified 1000\n",
      "Created a chunk of size 3114, which is longer than the specified 1000\n",
      "Created a chunk of size 1334, which is longer than the specified 1000\n",
      "Created a chunk of size 2117, which is longer than the specified 1000\n",
      "Created a chunk of size 1703, which is longer than the specified 1000\n",
      "Created a chunk of size 1068, which is longer than the specified 1000\n",
      "Created a chunk of size 1234, which is longer than the specified 1000\n",
      "Created a chunk of size 1059, which is longer than the specified 1000\n",
      "Created a chunk of size 1022, which is longer than the specified 1000\n",
      "Created a chunk of size 2356, which is longer than the specified 1000\n",
      "Created a chunk of size 1014, which is longer than the specified 1000\n",
      "Created a chunk of size 1055, which is longer than the specified 1000\n",
      "Created a chunk of size 1862, which is longer than the specified 1000\n",
      "Created a chunk of size 2411, which is longer than the specified 1000\n",
      "Created a chunk of size 1001, which is longer than the specified 1000\n",
      "Created a chunk of size 3141, which is longer than the specified 1000\n",
      "Created a chunk of size 1015, which is longer than the specified 1000\n",
      "Created a chunk of size 1715, which is longer than the specified 1000\n",
      "Created a chunk of size 1423, which is longer than the specified 1000\n",
      "Created a chunk of size 3145, which is longer than the specified 1000\n",
      "Created a chunk of size 1524, which is longer than the specified 1000\n",
      "Created a chunk of size 1582, which is longer than the specified 1000\n",
      "Created a chunk of size 1162, which is longer than the specified 1000\n",
      "Created a chunk of size 1244, which is longer than the specified 1000\n",
      "Created a chunk of size 1332, which is longer than the specified 1000\n",
      "Created a chunk of size 1425, which is longer than the specified 1000\n",
      "Created a chunk of size 2151, which is longer than the specified 1000\n",
      "Created a chunk of size 2660, which is longer than the specified 1000\n",
      "Created a chunk of size 1278, which is longer than the specified 1000\n",
      "Created a chunk of size 1388, which is longer than the specified 1000\n",
      "Created a chunk of size 1293, which is longer than the specified 1000\n",
      "Created a chunk of size 1352, which is longer than the specified 1000\n",
      "Created a chunk of size 1526, which is longer than the specified 1000\n",
      "Created a chunk of size 1707, which is longer than the specified 1000\n",
      "Created a chunk of size 1095, which is longer than the specified 1000\n",
      "Created a chunk of size 1152, which is longer than the specified 1000\n",
      "Created a chunk of size 1440, which is longer than the specified 1000\n",
      "Created a chunk of size 2005, which is longer than the specified 1000\n",
      "Created a chunk of size 2519, which is longer than the specified 1000\n",
      "Created a chunk of size 1167, which is longer than the specified 1000\n",
      "Created a chunk of size 1282, which is longer than the specified 1000\n",
      "Created a chunk of size 1477, which is longer than the specified 1000\n",
      "Created a chunk of size 1636, which is longer than the specified 1000\n",
      "Created a chunk of size 2272, which is longer than the specified 1000\n",
      "Created a chunk of size 2361, which is longer than the specified 1000\n",
      "Created a chunk of size 1063, which is longer than the specified 1000\n",
      "Created a chunk of size 1282, which is longer than the specified 1000\n",
      "Created a chunk of size 2152, which is longer than the specified 1000\n",
      "Created a chunk of size 1109, which is longer than the specified 1000\n",
      "Created a chunk of size 1283, which is longer than the specified 1000\n",
      "Created a chunk of size 1078, which is longer than the specified 1000\n",
      "Created a chunk of size 1050, which is longer than the specified 1000\n",
      "Created a chunk of size 1395, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# Chunk the files \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://longbaoit/Torch-Pruning-embeddings already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [00:36<00:00, 36.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (100, 1536)  float32   None   \n",
      "    id        text      (100, 1)      str     None   \n",
      " metadata     json      (100, 1)      str     None   \n",
      "   text       text      (100, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [00:45<00:00, 45.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (200, 1536)  float32   None   \n",
      "    id        text      (200, 1)      str     None   \n",
      " metadata     json      (200, 1)      str     None   \n",
      "   text       text      (200, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [00:43<00:00, 43.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (300, 1536)  float32   None   \n",
      "    id        text      (300, 1)      str     None   \n",
      " metadata     json      (300, 1)      str     None   \n",
      "   text       text      (300, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [01:12<00:00, 72.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (400, 1536)  float32   None   \n",
      "    id        text      (400, 1)      str     None   \n",
      " metadata     json      (400, 1)      str     None   \n",
      "   text       text      (400, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [01:48<00:00, 108.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (500, 1536)  float32   None   \n",
      "    id        text      (500, 1)      str     None   \n",
      " metadata     json      (500, 1)      str     None   \n",
      "   text       text      (500, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [01:58<00:00, 118.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (600, 1536)  float32   None   \n",
      "    id        text      (600, 1)      str     None   \n",
      " metadata     json      (600, 1)      str     None   \n",
      "   text       text      (600, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [01:55<00:00, 115.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (700, 1536)  float32   None   \n",
      "    id        text      (700, 1)      str     None   \n",
      " metadata     json      (700, 1)      str     None   \n",
      "   text       text      (700, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [01:50<00:00, 110.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (800, 1536)  float32   None   \n",
      "    id        text      (800, 1)      str     None   \n",
      " metadata     json      (800, 1)      str     None   \n",
      "   text       text      (800, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [01:48<00:00, 108.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (900, 1536)  float32   None   \n",
      "    id        text      (900, 1)      str     None   \n",
      " metadata     json      (900, 1)      str     None   \n",
      "   text       text      (900, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [01:12<00:00, 72.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape       dtype  compression\n",
      "  -------    -------     -------     -------  ------- \n",
      " embedding  embedding  (1000, 1536)  float32   None   \n",
      "    id        text      (1000, 1)      str     None   \n",
      " metadata     json      (1000, 1)      str     None   \n",
      "   text       text      (1000, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [01:14<00:00, 74.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape       dtype  compression\n",
      "  -------    -------     -------     -------  ------- \n",
      " embedding  embedding  (1100, 1536)  float32   None   \n",
      "    id        text      (1100, 1)      str     None   \n",
      " metadata     json      (1100, 1)      str     None   \n",
      "   text       text      (1100, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 100 embeddings in 1 batches of size 100:: 100%|██████████| 1/1 [01:08<00:00, 68.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape       dtype  compression\n",
      "  -------    -------     -------     -------  ------- \n",
      " embedding  embedding  (1200, 1536)  float32   None   \n",
      "    id        text      (1200, 1)      str     None   \n",
      " metadata     json      (1200, 1)      str     None   \n",
      "   text       text      (1200, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 1240 embeddings in 3 batches of size 500::   0%|          | 0/3 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-small in organization org-AwbQrlmqRcHLRc8EQLdCyz8Q on tokens per min (TPM): Limit 150000, Requested 209991. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch):\n\u001b[0;32m      6\u001b[0m     db\u001b[38;5;241m.\u001b[39madd_documents(texts[b \u001b[38;5;241m*\u001b[39m batch: (b \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch])\n\u001b[1;32m----> 7\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\langchain_core\\vectorstores.py:138\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    137\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\langchain_community\\vectorstores\\deeplake.py:260\u001b[0m, in \u001b[0;36mDeepLake.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`texts` parameter shouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    261\u001b[0m         text\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    262\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    263\u001b[0m         embedding_data\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    264\u001b[0m         embedding_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    265\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         return_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SampleExtendError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to append a sample to the tensor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\deeplake\\core\\vectorstore\\deeplake_vectorstore.py:229\u001b[0m, in \u001b[0;36mVectorStore.add\u001b[1;34m(self, embedding_function, embedding_data, embedding_tensor, return_ids, rate_limiter, **tensors)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    152\u001b[0m     embedding_function: Optional[Union[Callable, List[Callable]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtensors,\n\u001b[0;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Adding elements to deeplake vector store.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    Tensor names are specified as parameters, and data for each tensor is specified as parameter values. All data must of equal length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m        Optional[List[str]]: List of ids if ``return_ids`` is set to True. Otherwise, None.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_handler\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    230\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[0;32m    231\u001b[0m         embedding_data\u001b[38;5;241m=\u001b[39membedding_data,\n\u001b[0;32m    232\u001b[0m         embedding_tensor\u001b[38;5;241m=\u001b[39membedding_tensor,\n\u001b[0;32m    233\u001b[0m         return_ids\u001b[38;5;241m=\u001b[39mreturn_ids,\n\u001b[0;32m    234\u001b[0m         rate_limiter\u001b[38;5;241m=\u001b[39mrate_limiter,\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtensors,\n\u001b[0;32m    236\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\deeplake\\core\\vectorstore\\dataset_handlers\\client_side_dataset_handler.py:139\u001b[0m, in \u001b[0;36mClientSideDH.add\u001b[1;34m(self, embedding_function, embedding_data, embedding_tensor, return_ids, rate_limiter, **tensors)\u001b[0m\n\u001b[0;32m    133\u001b[0m processed_tensors, id_ \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mpreprocess_tensors(\n\u001b[0;32m    134\u001b[0m     embedding_data, embedding_tensor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtensors\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m id_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend_or_ingest_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrate_limiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrate_limiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\dataset\\dataset.py:535\u001b[0m, in \u001b[0;36mextend_or_ingest_dataset\u001b[1;34m(processed_tensors, dataset, embedding_function, embedding_tensor, embedding_data, rate_limiter, logger)\u001b[0m\n\u001b[0;32m    533\u001b[0m rate_limiter \u001b[38;5;241m=\u001b[39m populate_rate_limiter(rate_limiter)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# TODO: Add back the old logic with checkpointing after indexing is fixed\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m \u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrate_limiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\dataset\\dataset.py:481\u001b[0m, in \u001b[0;36mextend\u001b[1;34m(embedding_function, embedding_data, embedding_tensor, processed_tensors, dataset, rate_limiter, _extend_batch_size, logger)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(embedding_data[\u001b[38;5;241m0\u001b[39m]), _extend_batch_size),\n\u001b[0;32m    477\u001b[0m     progressbar_str,\n\u001b[0;32m    478\u001b[0m ):\n\u001b[0;32m    479\u001b[0m     batch_start, batch_end \u001b[38;5;241m=\u001b[39m idx, idx \u001b[38;5;241m+\u001b[39m _extend_batch_size\n\u001b[1;32m--> 481\u001b[0m     batched_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43m_compute_batched_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrate_limiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m     batched_tensors \u001b[38;5;241m=\u001b[39m _slice_non_embedding_tensors(\n\u001b[0;32m    491\u001b[0m         processed_tensors, embedding_tensor, batch_start, batch_end\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    494\u001b[0m     batched_processed_tensors \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatched_embeddings, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatched_tensors}\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\dataset\\dataset.py:422\u001b[0m, in \u001b[0;36m_compute_batched_embeddings\u001b[1;34m(embedding_function, embedding_data, embedding_tensor, start_idx, end_idx, rate_limiter)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, data, tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embedding_function, embedding_data, embedding_tensor):\n\u001b[0;32m    421\u001b[0m     data_slice \u001b[38;5;241m=\u001b[39m data[start_idx:end_idx]\n\u001b[1;32m--> 422\u001b[0m     embedded_data \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate_limiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrate_limiter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    425\u001b[0m         return_embedded_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(embedded_data)\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\deeplake\\core\\vectorstore\\embeddings\\embedder.py:77\u001b[0m, in \u001b[0;36mDeepLakeEmbedder.embed_documents\u001b[1;34m(self, documents, rate_limiter)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rate_limiter[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_rate_limiter(documents, embedding_func, rate_limiter)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membedding_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:668\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:494\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    492\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 494\u001b[0m     response \u001b[38;5;241m=\u001b[39m embed_with_retry(\n\u001b[0;32m    495\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size],\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    500\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:116\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\openai\\resources\\embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\openai\\_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\openai\\_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\OneDrive\\Máy tính\\Project\\Python\\chat-with-code\\.venv\\lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for text-embedding-3-small in organization org-AwbQrlmqRcHLRc8EQLdCyz8Q on tokens per min (TPM): Limit 150000, Requested 209991. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "username = 'longbaoit'\n",
    "db = DeepLake(dataset_path=f'hub://{username}/Torch-Pruning-embeddings', embedding=embeddings)\n",
    "\n",
    "batch = 100\n",
    "for b in range(len(texts) // batch):\n",
    "    db.add_documents(texts[b * batch: (b + 1) * batch])\n",
    "db.add_documents(texts[len(texts) // batch:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 52 embeddings in 1 batches of size 52:: 100%|██████████| 1/1 [02:32<00:00, 152.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://longbaoit/Torch-Pruning-embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape       dtype  compression\n",
      "  -------    -------     -------     -------  ------- \n",
      " embedding  embedding  (1252, 1536)  float32   None   \n",
      "    id        text      (1252, 1)      str     None   \n",
      " metadata     json      (1252, 1)      str     None   \n",
      "   text       text      (1252, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a6fbfa3f-09f5-11ef-bc0d-88d82e2767cf',\n",
       " 'a6fbfa40-09f5-11ef-a926-88d82e2767cf',\n",
       " 'a6fbfa41-09f5-11ef-9f30-88d82e2767cf',\n",
       " 'a6fbfa42-09f5-11ef-9be2-88d82e2767cf',\n",
       " 'a6fbfa43-09f5-11ef-9a43-88d82e2767cf',\n",
       " 'a6fbfa44-09f5-11ef-9ddc-88d82e2767cf',\n",
       " 'a6fbfa45-09f5-11ef-ae07-88d82e2767cf',\n",
       " 'a6fbfa46-09f5-11ef-9dc4-88d82e2767cf',\n",
       " 'a6fbfa47-09f5-11ef-b70c-88d82e2767cf',\n",
       " 'a6fbfa48-09f5-11ef-906e-88d82e2767cf',\n",
       " 'a6fbfa49-09f5-11ef-af21-88d82e2767cf',\n",
       " 'a6fbfa4a-09f5-11ef-b68a-88d82e2767cf',\n",
       " 'a6fbfa4b-09f5-11ef-8c49-88d82e2767cf',\n",
       " 'a6fbfa4c-09f5-11ef-9cb1-88d82e2767cf',\n",
       " 'a6fbfa4d-09f5-11ef-847a-88d82e2767cf',\n",
       " 'a6fbfa4e-09f5-11ef-be9a-88d82e2767cf',\n",
       " 'a6fbfa4f-09f5-11ef-beb3-88d82e2767cf',\n",
       " 'a6fbfa50-09f5-11ef-9a65-88d82e2767cf',\n",
       " 'a6fbfa51-09f5-11ef-b853-88d82e2767cf',\n",
       " 'a6fbfa52-09f5-11ef-9d88-88d82e2767cf',\n",
       " 'a6fbfa53-09f5-11ef-b96e-88d82e2767cf',\n",
       " 'a6fbfa54-09f5-11ef-a6b6-88d82e2767cf',\n",
       " 'a6fbfa55-09f5-11ef-af3e-88d82e2767cf',\n",
       " 'a6fbfa56-09f5-11ef-b7fb-88d82e2767cf',\n",
       " 'a6fbfa57-09f5-11ef-bdc0-88d82e2767cf',\n",
       " 'a6fbfa58-09f5-11ef-b637-88d82e2767cf',\n",
       " 'a6fbfa59-09f5-11ef-a8ed-88d82e2767cf',\n",
       " 'a6fbfa5a-09f5-11ef-be5b-88d82e2767cf',\n",
       " 'a6fbfa5b-09f5-11ef-805c-88d82e2767cf',\n",
       " 'a6fbfa5c-09f5-11ef-8b2f-88d82e2767cf',\n",
       " 'a6fbfa5d-09f5-11ef-b3c7-88d82e2767cf',\n",
       " 'a6fbfa5e-09f5-11ef-8c7d-88d82e2767cf',\n",
       " 'a6fbfa5f-09f5-11ef-b2c4-88d82e2767cf',\n",
       " 'a6fbfa60-09f5-11ef-abaf-88d82e2767cf',\n",
       " 'a6fbfa61-09f5-11ef-9546-88d82e2767cf',\n",
       " 'a6fbfa62-09f5-11ef-b772-88d82e2767cf',\n",
       " 'a6fbfa63-09f5-11ef-b702-88d82e2767cf',\n",
       " 'a6fbfa64-09f5-11ef-8c99-88d82e2767cf',\n",
       " 'a6fbfa65-09f5-11ef-8b83-88d82e2767cf',\n",
       " 'a6fbfa66-09f5-11ef-881c-88d82e2767cf',\n",
       " 'a6fbfa67-09f5-11ef-b3a5-88d82e2767cf',\n",
       " 'a6fbfa68-09f5-11ef-9ab6-88d82e2767cf',\n",
       " 'a6fbfa69-09f5-11ef-9fd0-88d82e2767cf',\n",
       " 'a6fbfa6a-09f5-11ef-b87f-88d82e2767cf',\n",
       " 'a6fbfa6b-09f5-11ef-bc1d-88d82e2767cf',\n",
       " 'a6fbfa6c-09f5-11ef-aff2-88d82e2767cf',\n",
       " 'a6fbfa6d-09f5-11ef-ada6-88d82e2767cf',\n",
       " 'a6fbfa6e-09f5-11ef-8d33-88d82e2767cf',\n",
       " 'a6fbfa6f-09f5-11ef-b6e1-88d82e2767cf',\n",
       " 'a6fbfa70-09f5-11ef-a676-88d82e2767cf',\n",
       " 'a6fbfa71-09f5-11ef-baae-88d82e2767cf',\n",
       " 'a6fbfa72-09f5-11ef-8ad7-88d82e2767cf']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add_documents(texts[(len(texts) // batch) * batch:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://longbaoit/Torch-Pruning-embeddings already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "db = DeepLake(dataset_path=f'hub://{username}/Torch-Pruning-embeddings', read_only=True, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['fetch_k'] = 100\n",
    "# retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tQuestion: How to prune a network by using Torch-Pruning library?\n",
      "\n",
      "\tAnswer: To prune a network using the Torch-Pruning library, you can follow these general steps:\n",
      "\n",
      "1. **Build a Dependency Graph**: Use the `DependencyGraph` class to build a dependency graph for your model. This step involves running your model with example inputs to track the network computation graph and record dependencies between layers.\n",
      "\n",
      "2. **Specify Pruning Dimensions**: Define the dimensions you want to prune in your model. For example, you can specify which channels to prune in a convolutional layer.\n",
      "\n",
      "3. **Get Pruning Groups**: Use the `get_pruning_group` method from the `DependencyGraph` to identify all coupled layers when pruning a specific layer. This method returns a `tp.Group` object containing information about the coupled layers.\n",
      "\n",
      "4. **Apply Pruning**: Once you have identified the pruning groups, you can apply the pruning to the model based on the desired criteria.\n",
      "\n",
      "Here is a minimal example to illustrate these steps:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from torchvision.models import resnet18\n",
      "import torch_pruning as tp\n",
      "\n",
      "model = resnet18(pretrained=True).eval()\n",
      "\n",
      "# 1. Build a Dependency Graph\n",
      "DG = tp.DependencyGraph()\n",
      "DG.build_dependency(model, example_inputs=torch.randn(1,3,224,224))\n",
      "\n",
      "# 2. Specify Pruning Dimensions\n",
      "pruning_idxs = [2, 6, 9]  # Example indices of channels to prune\n",
      "pruning_group = DG.get_pruning_group(model.conv1, tp.prune_conv_out_channels, idxs=pruning_idxs)\n",
      "\n",
      "print(pruning_group.details())  # Print details of the pruning group\n",
      "\n",
      "# 3. Apply Pruning\n",
      "# You can now apply the pruning to the model based on the identified pruning groups\n",
      "```\n",
      "\n",
      "This is a basic example to get you started with pruning a network using Torch-Pruning. For more advanced pruning techniques and customization options, you can refer to the library's documentation and tutorials.\n",
      "\n",
      "\tQuestion: Which pruning method does Torch-Pruning library support?\n",
      "\n",
      "\tAnswer: The Torch-Pruning library supports various pruning methods, including:\n",
      "- MagnitudePruner\n",
      "- BNScalePruner\n",
      "- GroupNormPruner\n",
      "- GrowingRegPruner\n",
      "- RandomPruner\n",
      "\n",
      "These pruning methods are part of the high-level pruners available in the library.\n",
      "\n",
      "\tQuestion: What pruning method is appropriate to each common network such as CNN, RNN, MLP?\n",
      "\n",
      "\tAnswer: The choice of pruning method for common networks like CNNs, RNNs, and MLPs can vary based on the specific characteristics of each network and the goals of pruning. Here are some general guidelines for pruning methods that are commonly used for each type of network:\n",
      "\n",
      "1. **Convolutional Neural Networks (CNNs)**:\n",
      "   - **Filter Pruning**: Removing entire filters (also known as channels) from convolutional layers based on certain criteria like weight magnitude, activation values, or importance scores.\n",
      "   - **Structured Pruning**: Pruning structured patterns within convolutional layers to exploit the inherent structure of CNNs.\n",
      "   - **Group Pruning**: Pruning groups of filters or channels together to maintain the integrity of feature maps.\n",
      "   - **L1 Regularization**: Using L1 regularization to encourage sparsity in the weights of convolutional layers.\n",
      "\n",
      "2. **Recurrent Neural Networks (RNNs)**:\n",
      "   - **Unit Pruning**: Removing entire units (neurons) from RNN layers based on importance scores or other criteria.\n",
      "   - **Connection Pruning**: Pruning connections between units in RNNs to reduce computational complexity.\n",
      "   - **Magnitude-based Pruning**: Pruning weights in RNN layers based on their magnitudes or importance values.\n",
      "   - **Structured Pruning**: Pruning structured patterns within RNN layers to exploit the sequential nature of RNNs.\n",
      "\n",
      "3. **Multi-Layer Perceptrons (MLPs)**:\n",
      "   - **Weight Pruning**: Pruning individual weights in MLP layers based on certain criteria like weight magnitude or importance scores.\n",
      "   - **Unit Pruning**: Removing entire neurons or units from MLP layers to reduce model size.\n",
      "   - **Layer Pruning**: Pruning entire layers in MLPs based on their contribution to the overall model performance.\n",
      "   - **Global Pruning**: Applying a uniform pruning ratio across all layers of the MLP.\n",
      "\n",
      "It's important to note that the effectiveness of pruning methods can vary depending on the specific architecture of the network, the dataset being used, and the pruning goals (e.g., reducing model size, improving inference speed, maintaining accuracy). Experimentation and evaluation are key to determining the most appropriate pruning method for a given network.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = ['How to prune a network by using Torch-Pruning library?',\n",
    "             'Which pruning method does Torch-Pruning library support?',\n",
    "             'What pruning method is appropriate to each common network such as CNN, RNN, MLP?']\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({'question': question, 'chat_history': chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    print(f\"\\tQuestion: {question}\\n\")\n",
    "    print(f\"\\tAnswer: {result['answer']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
